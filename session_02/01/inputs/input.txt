Storing, managing, and processing datasets are foundational to both computer
science and data science. The enormous size of today's data sets and the
specific requirements of modern applications, necessitated the growth of a new
generation of data management systems, where the emphasis is put on
distributed and fault-tolerant processing. New programming paradigms have
evolved, an abundance of information platforms offering data management and
analysis solutions appeared and a number of novel methods and tools have been
developed. This course introduces the fundamentals of big data storage,
retrieval, and processing systems. As these fundamentals are introduced,
exemplary technologies are used to illustrate how big data systems can
leverage very large data sets that become available through multiple sources
and are characterized by diverse levels of volume (terabytes; billion
records), velocity (batch; real-time; streaming) and variety (structured;
semi-structured; unstructured). The course aims to provide students with both
theoretical knowledge and practical experience of the field by covering recent
research on big data systems and their basic properties. Students consider
both small and large datasets because both are equally important and justify
different trade-offs. Topics include: software frameworks for distributed
storage and processing of very large data sets, MapReduce programming model,
querying of structured data sets, column stores, key-value stores, document
stores, graph databases, distributed stream processing frameworks.
